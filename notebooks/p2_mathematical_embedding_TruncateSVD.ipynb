{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Environment setup\n",
    "# -------------------------------------------------------------\n",
    "BASE_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------------------\n",
    "from src.utils.config import load_config\n",
    "config = load_config(base_dir=BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.decomposition import TruncatedSVD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical embedding - truncateSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to_dask_array \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# === Leer CSV ===\n",
    "individual_tensors = config[\"paths\"][\"tensors_convolution\"]\n",
    "data = dd.read_csv(individual_tensors)\n",
    "\n",
    "row_id = data.iloc[:, 0]                     # primera columna\n",
    "df_num = data.iloc[:, 1:].astype(float)      # num√©ricas\n",
    "\n",
    "print(\" to_dask_array \")\n",
    "X = df_num.to_dask_array(lengths=True)\n",
    "\n",
    "# === Escalado (Dask-ML) ===\n",
    "print(\" Escalado (Dask-ML) \")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# === SVD sin cargar en memoria ===\n",
    "print(\" SVD \")\n",
    "n_components = 20\n",
    "tsvd = TruncatedSVD(n_components=n_components)\n",
    "X_reduced = tsvd.fit_transform(X_scaled)\n",
    "\n",
    "# Convertir resultado a Dask-DF sin computar en RAM\n",
    "print(\" to dask \")\n",
    "svd_cols = [f\"svd_{i}\" for i in range(n_components)]\n",
    "df_svd = dd.from_dask_array(X_reduced, columns=svd_cols)\n",
    "\n",
    "# Concatenar IDs + componentes\n",
    "print(\" concat \")\n",
    "result = dd.concat([\n",
    "    row_id.reset_index(drop=True),\n",
    "    df_svd.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# === Guardar sin computar en RAM ===\n",
    "print(\" Write \")\n",
    "tensors_svd = config[\"paths\"][\"tensors_svd\"]\n",
    "os.makedirs(os.path.dirname(tensors_svd), exist_ok=True)\n",
    "result.to_parquet(tensors_svd, write_index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"‚è∞ Start: {time.ctime(start_time)}\")\n",
    "print(f\"üèÅ End:   {time.ctime(end_time)}\")\n",
    "print(f\"‚è±Ô∏è Total: {end_time - start_time:.2f} sec ({(end_time - start_time)/60:.2f} min)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| M√©todo de escalado      | Qu√© hace                                 | Preserva direcci√≥n | Ideal para Cosine | Ideal para Euclidean | Ideal para Manhattan (L1) | Ideal para Minkowski | Adecuado para K-Means | Comentarios clave |\n",
    "|--------------------------|-------------------------------------------|---------------------|--------------------|------------------------|---------------------------|------------------------|------------------------|--------------------|\n",
    "| **L2 Normalization**     | Normaliza cada vector a norma 1 (‚Äñx‚Äñ‚ÇÇ=1) | ‚úî S√≠               | ‚≠ê **S√≠** (equivalencia exacta) | ‚úî S√≠ (si magnitud no importa) | ‚ùå No (distorsiona L1) | ‚ùå No | ‚≠ê **S√≠ (cuando quieres imitar Cosine-KMeans)** | Convierte Euclidean en Cosine; ideal para embeddings y perfiles. |\n",
    "| **StandardScaler**       | Centra y escala cada feature (z-score)   | ‚ùå No              | ‚ùå **No** | ‚≠ê **S√≠** (distancia euclidiana cl√°sica) | ‚≠ê **S√≠** | ‚≠ê **S√≠** | ‚≠ê **S√≠** | Mantiene variancia comparable entre features; est√°ndar para ML tradicional. |\n",
    "| **MinMaxScaler**         | Escala cada feature a [0,1]              | ‚ùå No              | ‚ùå No | ‚≠ê S√≠ (cuando las escalas importan) | ‚≠ê S√≠ | ‚≠ê S√≠ | ‚≠ê S√≠ | Preserva relaciones relativas; √∫til en clustering basado en distancias mixtas. |\n",
    "| **RobustScaler**         | Escala usando medianas y IQR             | ‚ùå No              | ‚ùå No | ‚≠ê S√≠ (robusto a outliers) | ‚≠ê S√≠ | ‚≠ê S√≠ | ‚≠ê S√≠ | Excelente cuando hay outliers fuertes; no apto para coseno. |\n",
    "| **None (sin escala)**    | Deja los datos crudos                    | A veces            | ‚ùå No | ‚ùå No (si las escalas var√≠an entre features) | ‚ùå No | ‚ùå No | ‚ùå No | Solo √∫til si todas las features ya est√°n en la misma escala y sin outliers. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# individual_tensors = config[\"paths\"][\"tensors_convolution\"]\n",
    "tensors_svd = config[\"paths\"][\"tensors_svd\"]\n",
    "inpust_path = tensors_svd\n",
    "data = dd.read_parquet(inpust_path)\n",
    "data = data.repartition(npartitions=data.npartitions)\n",
    "\n",
    "id_col = \"row_id\"\n",
    "value_cols = [c for c in data.columns if c != id_col]\n",
    "\n",
    "X = data[value_cols].astype(float).to_dask_array(lengths=True)\n",
    "row_ids = data[id_col].to_dask_array(lengths=True).reshape(-1, 1)\n",
    "\n",
    "L1_norms = da.sum(da.abs(X), axis=1, keepdims=True)\n",
    "L1_norms = da.where(L1_norms == 0, 1, L1_norms)\n",
    "X_norm_L1 = X / L1_norms\n",
    "\n",
    "full_array = da.hstack([row_ids, X_norm_L1])\n",
    "df_norm_L1 = dd.from_dask_array(full_array, columns=[id_col] + value_cols)\n",
    "\n",
    "output_path_L1 = \"outputs/tensors_normalized_L1.parquet\"\n",
    "os.makedirs(os.path.dirname(tensors_svd), exist_ok=True)\n",
    "df_norm_L1.to_parquet(output_path_L1, write_index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"‚è∞ Start: {time.ctime(start_time)}\")\n",
    "print(f\"üèÅ End:   {time.ctime(end_time)}\")\n",
    "print(f\"‚è±Ô∏è Total: {end_time - start_time:.2f} sec ({(end_time - start_time)/60:.2f} min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# individual_tensors = config[\"paths\"][\"tensors_convolution\"]\n",
    "tensors_svd = config[\"paths\"][\"tensors_svd\"]\n",
    "inpust_path = tensors_svd\n",
    "data = dd.read_parquet(inpust_path)\n",
    "data = data.repartition(npartitions=data.npartitions)\n",
    "\n",
    "id_col = \"row_id\"\n",
    "value_cols = [c for c in data.columns if c != id_col]\n",
    "\n",
    "X = data[value_cols].astype(float).to_dask_array(lengths=True)\n",
    "row_ids = data[id_col].to_dask_array(lengths=True).reshape(-1, 1)\n",
    "\n",
    "L2_norms = da.linalg.norm(X, axis=1, keepdims=True)\n",
    "L2_norms = da.where(L2_norms == 0, 1, L2_norms)\n",
    "X_norm_L2 = X / L2_norms\n",
    "\n",
    "full_array = da.hstack([row_ids, X_norm_L2])\n",
    "df_norm_L2 = dd.from_dask_array(full_array, columns=[id_col] + value_cols)\n",
    "\n",
    "output_path_L2 = \"outputs/tensors_normalized_L2.parquet\"\n",
    "os.makedirs(os.path.dirname(tensors_svd), exist_ok=True)\n",
    "df_norm_L2.to_parquet(output_path_L2, write_index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"‚è∞ Start: {time.ctime(start_time)}\")\n",
    "print(f\"üèÅ End:   {time.ctime(end_time)}\")\n",
    "print(f\"‚è±Ô∏è Total: {end_time - start_time:.2f} sec ({(end_time - start_time)/60:.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "consumer-3d-segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "280px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
